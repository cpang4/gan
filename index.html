<!DOCTYPE HTML>
<!--
	Story by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<style>
.tabcontent {
  display: none;
  margin-top: 1rem;
  margin-bottom: 1rem;
}
.tabcontent2 {
  display: none;
  margin-top: 1rem;
  margin-bottom: 1rem;
}
</style>
<html>
	<head>
		<title>Generative Adversarial Networks</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper" class="divided">

				<!-- Banner -->
					<section class="banner style5 content-align-center" style="background-image:url(content/banner.png)">
						<div class="content">
							<h1>Generative Adversarial Networks</h1>
							<p><b>Google Applied Machine Learning Intensive</b>
								<br><i>Mills College, Spring 2019</i>
								<br><a href="https://www.linkedin.com/in/clairepang/" target="_blank"><b>Claire Pang</b></a>, University of San Francisco
								<br><a href="https://www.linkedin.com/in/kyle-astroth/" target="_blank"><b>Kyle Astroth</b></a>, Mills College
								<br><a href="https://www.linkedin.com/in/ajbethel/" target="_blank"><b>AJ Bethel</b></a>, Texas Woman's University</p>
						</div>
					</section>

					<section class="spotlight style1">
						<div class="content">
							<h2>What are GANs?</h2>
							<p align = "justify"><b>G</b>enerative <b>A</b>dversarial <b>N</b>etworks (GANs) consist of two neural networks that are competing against each other. One neural network, the “generator” takes a random noise vector to produce fake images. The other network, the “discriminator” is fed real images, and uses those to determine if the fake images made by the generator are real or fake. The generator relies on feedback from the discriminator to get better at creating images, while the discriminator gets better at classifying between real and fake images. In our implementation, our generator and discriminator will be convolutional neural networks.
								<br><br>A popular metaphor is an art forger and art critic:</p>
								<img src = "./content/artcritic.png" height = "100%" width = "100%"/>
								<p align = "justify">More specifically, the discriminator is fed (real) training images and learns to distinguish between what's real and what's fake. The generator is given a <i>z</i>-dimensional random noise vector and uses that to generate an image. This picture is then sent to the discriminator to return a value 1 (if real) or 0 (if fake). Once compared against the labels (ground truth: real or fake), the losses are computed. The generator and discriminator are then updated; thus over time, both get better at their jobs.</p>
								<img src = "./content/gen-dis.jpg" height = "100%" width="100%"/>
								<p align = "justify">It's important that discriminator doesn't start off too strong, otherwise the generator won't get any information on how to improve itself. The generator doesn't know what the real data looks like; it relies on the discriminator's feedback. Only when the discriminator and generator are balanced do both improve, and hence lead to the generator producing realistic images.
						</div>
					</section>

					<section class="spotlight style1">
						<div class="content">
							<h2>Architecture of our DCGAN</h2>
							<p align="justify">The model used for this project was coded using Python, TensorFlow, and Nvidia’s CUDA toolkit. Due to the computational power required to train a GAN, we ran instances of our model using GPUs - both locally (Nvidia RTX 2060) and using Google Cloud Platform (Nvidia Tesla K80).</p>

							<p align="justify">Much of our code was based on <a href = "https://github.com/simoninithomas/CatDCGAN/blob/master/Cat%20DCGAN.ipynb" target="_blank">this notebook</a> by <a href = "https://github.com/simoninithomas" target = "_blank">simoninithomas</a>. A corresponding tutorial, which goes more in depth about the code, is also available <a href = "https://medium.freecodecamp.org/how-ai-can-learn-to-generate-pictures-of-cats-ba692cb6eae4" target = "_blank">here</a>. GANs are difficult to train and sensitive to hyperparameters (you don’t want the generator or discriminator overpowering each other). We tried the hyperparameters used in this notebook, as well as our own. The hyperparameters we tuned were: generator learning rate, discriminator learning rate, batch size, alpha (leak parameter for leaky ReLU), and beta (exponential decay rate for 1st moment in the optimizer). For batch size, it was required to use 32 when trained locally to avoid OOM errors.
							<br>The number of epochs run depended on how long we let the model train for, and whether we thought (based on the outputted images) the model was improving/getting worse. For more specific hyperparameters, please see our results below.</p>

							<p align="justify">Our GAN consisted of two convolutional neural networks pitted against each other. The architecture used for this project is as follows:</p>
							<img src = "./content/architecture.png" height = "100%" width = "100%"/>
							<p align = "justify">From these <a href="https://github.com/soumith/ganhacks" target = "_blank">"GAN hacks"</a> it is recommended that:
								<ul>
									<li>Batch normalization occurs at each layer <i>(except for first layer of generator)</i></li>
									<li>LeakyReLU as the activation function for both generator and discriminator</li>
									<li>Use Adam optimizer</li>
									<li>tanh as last layer of generator output</li>
								</ul>
							</p>
						</div>
					</section>

					<section class="spotlight style1">
						<div class="content">
							<h2>Results</h2>
							<p align="justify">We trained multiple GANs on different datasets, and the categories that we're satisified with the results are listed below. The code used resizes images to 128x128 and generates 128x128 sized images (may appear smaller on the website here). We modified the code as to save a version of the model every 50 epochs and generate a 3x3 grid of images after each epoch. Our training process, as well as a curated collection of our best generated images, are below.</p>
							<div class="tab">
								<button class="tablinks" id = "sunsetsbutton" onclick="openResults(event, 'sunsets')">Sunsets</button>
							  	<button class="tablinks" id = "flowersbutton" onclick="openResults(event, 'flowers')">Flowers</button>
							  	<button class="tablinks" id = "facesbutton" onclick="openResults(event, 'faces')">Faces</button>
							</div>


							<div id="flowers" class="tabcontent">
								<h3>The Dataset</h3>
								<p align = "justify">This dataset consists of 1,487 flower paintings from wikiart.org and Google images. We included only paintings of flowers in vases.</p>
								<h3>Training process</h3>
								<p align = "justify"><i>GPU:</i> Nvidia RTX 2060, <i>learning rate:</i> generator - 0.00002, discriminator - 0.000005, <i>alpha: 0.2</i>, <i>beta</i>: 0.5</p>
								<img src = "./content/flowers-train.png" width = "100%" height = "100%"/>
								<p align="right"><a href = "flowers.html" target = "blank">jupyter notebook with code + full training</a></p>
								<h3>Other generated images</h3>
								<p align = "justify">The following images are generated using the model saved at epoch 500.</p>
								<img src = "./content/merge-flowers.jpg" width = "100%"/>
							</div>
							<div id="faces" class="tabcontent">
							  <h3>The Dataset</h3>
								<p align = "justify">This dataset consists of 3,785 faces from the <a href = "https://github.com/NVlabs/ffhq-dataset" target = "blank">FFHQ</a> dataset. We used a subset of the 70,000 128x128 thumbnails dataset. The data was cleaned to include adults whose heads were facing forward with little to no obstruction of the face (caused by hats, sunglasses, etc). Photos that were eliminated from the dataset also included anomalies like: more than one person in the photo, and off-kilter photos where the adult pictured did not represent a face-forward headshot. We tried training a model with uncleaned data, and the results were much worse.</p>
								<h3>Training process</h3>
								<p align = "justify"><i>GPU:</i> Nvidia RTX 2060, <i>learning rate:</i> generator - 0.0002, discriminator - 0.00005, <i>alpha: 0.2</i>, <i>beta</i>: 0.5</p>
								<img src = "./content/faces-train.png" width = "100%" height = "100%"/>
								<h3>Other generated images</h3>
								<p align = "justify">The following images are generated using the model saved at epoch 500.</p>
								<img src = "./content/merge-faces.jpg" width = "100%" />
							</div>
							<div id="sunsets" class="tabcontent">
							  <h3>The Dataset</h3>
								<p align = "justify">This dataset consists of 2,349 sunsets scraped using the Flickr API.</p>
								<h3>Training process</h3>
								<p align = "justify"><i>GPU:</i> Nvidia RTX 2060, <i>learning rate:</i> generator - 0.0002, discriminator - 0.00005, <i>alpha: 0.5</i>, <i>beta</i>: 0.5</p>
								<img src = "./content/sunset-train.png" width = "100%" height = "100%"/>
								<p align="right"><a href = "sunsets.html" target = "blank">jupyter notebook with code + full training</a></p>
								<h3>Other generated images</h3>
								<p align = "justify">The following images are generated using the model saved at epoch 450 (top row) and epoch 500 (bottom row).</p>
								<img src = "./content/sunsets-merge.png" width = "100%"/>
							</div>
						</div>
					</section>

					<script>
						document.getElementById('sunsets').style.display = "block";
						tabcontent = document.getElementsByClassName("tabcontent");
						document.getElementById('sunsetsbutton').style.backgroundColor = "#DCDCDC";

						function openResults(evt, type) {
						  var i, tabcontent, tablinks;
						  tabcontent = document.getElementsByClassName("tabcontent");
						  for (i = 0; i < tabcontent.length; i++) {
						    tabcontent[i].style.display = "none";
						    document.getElementById(tabcontent[i].id + "button").style.backgroundColor = 'transparent';
						  }
						  tablinks = document.getElementsByClassName("tablinks");
						  for (i = 0; i < tablinks.length; i++) {
						    tablinks[i].className = tablinks[i].className.replace(" active", "");
						  }
						  document.getElementById(type).style.display = "block";
						  evt.currentTarget.className += " active";
						  document.getElementById(type + "button").style.backgroundColor = "#DCDCDC";
						}
					</script>

					<section class="spotlight style1">
						<div class = "content">
							<h2>Other versions of GANs</h2>
							<p align = "justify">In addition to DCGANs, there are also various other implementations of GANs. For experimentation purposes, we also tried using a Wasserstein GAN (WGAN) on the same sunsets dataset to compare the results. The code we used is from <a href = "https://github.com/Goldesel23/DCGAN-for-Bird-Generation/blob/master/train_wgan.py" target = "blank">goldesel23.</a></p>
							<img src = "./content/wgan-train.png" width = "100%" height = "100%" />
							<p align = "justify">Based on the above results, we believe that our sunset DCGAN lead to better results. The <a href = "https://arxiv.org/pdf/1701.07875.pdf" target = "_blank">WGAN</a> is supposedly more stable and fixes problems like mode collapse, however a <a href = "https://arxiv.org/pdf/1711.10337.pdf" target = "_blank">paper from Google Brain</a> at NIPS 2018 found no evidence that other versions of GANs, including WGAN, consistently performed better than the <a href="https://arxiv.org/pdf/1406.2661.pdf" target = "_blank">original GAN</a>.
						</div>
					</section>

					<section class="spotlight style1">
						<div class = "content">
							<h2>Discussion, tuning hyperparameters, and other datasets</h2>
							<h3>Discussion</h3>
							<p align = "justify">How to best tune a GAN is an open research problem. Grid and random search for trying different hyperparmeters are not the best methods due to the computational power and time required to train a GAN. While doing preliminary research for this project, we were worried about tackling GANs because of the time required to train. Mostly, we only found unsure estimates for how long it took to train – hence, when showing our results, we include our actual training times to provide an estimate for what to expect. Additionally, we didn't find many resources for what <i>didn't</i> work, we mostly saw the final result but didn't see the process of getting there. Thus, we are including the below sections to talk about our entire process of working with GANs.
								<br>Overall, we noticed that larger datasets do make a difference – our faces dataset was the largest, and the generated images turned out the clearest as well.</p>
							<h3>Tuning hyperparameters</h3>
							<p align = "justify"><i>Dataset:</i> Same 1,487 flower paintings as used above</p>
							<img src = "./content/flowers-comparison.png" width = "100%" height = "100%"/>
							<p align = "justify">Model 3 is the model used in the above flower results section. Other models with different hyperparameters, were run and led to worse results.
							<br>The results from Model 1 at epochs 300/350 look similar to the results at epoch 500 in Model 3. However, we noticed that the flowers generated using epochs 300 and 350 with these hyperparameters resulted in a lesser quantity of images considered "good" and with less defined features. By letting it run to 500 epochs we experienced mode collapse (when all the generated images start to look the same - see epoch 500). As the generated images don't necessarily get better by training longer, we recommend saving multiple versions of the model during training.
							<br>Model 2 led to mode collapse quickly, which is likely due to the tuning of alpha and beta.</p>
							
							<h3>Working with other datasets</h3>

							<div class="tab">
								<button class="tablinks" id = "beachesbutton" onclick="openResults2(event, 'beaches')">Beaches</button>
							  	<button class="tablinks" id = "dolphinsbutton" onclick="openResults2(event, 'dolphins')">Dolphins</button>
							  	<button class="tablinks" id = "horsesbutton" onclick="openResults2(event, 'horses')">Horses</button>
							  	<button class="tablinks" id = "macawsbutton" onclick="openResults2(event, 'macaws')">Macaws</button>
							  	<button class="tablinks" id = "psychedelicbutton" onclick="openResults2(event, 'psychedelic')">Psychedelic</button>
							</div>


							<div id="beaches" class="tabcontent2">
								<h4>The Dataset</h4>
								<p align = "justify">This dataset consists of 1,149 images of beaches from Google Images.</p>
								<h4>Training process</h4>
								<p align = "justify"><i>GPU:</i> Nvidia RTX 2060, <i>learning rate:</i> generator - 0.00002, discriminator - 0.000005, <i>alpha: 0.2</i>, <i>beta</i>: 0.5</p>
								<img src = "./content/beaches-train.png" width = "100%" height = "100%"/>
								<p align = "justify">The beaches dataset was scraped from Google Images and consequently the data was extremely diversified. A “photography” tag and “teal” color tag were applied through Google’s search tools to present photographic representations of beaches. However, the images collected represented a number of different views of beaches, featuring details like palm trees, giant stone pillars, etc. Due to the relatively small size of the dataset (1,149 images), paired with extremely diversified data, the output was lesser than desired quality. We edited the learning rates and played with the extremely sensitive hyperparameters to no avail of a desired result.</p>
							</div>

							<div id="dolphins" class="tabcontent2">
								<h4>The Dataset</h4>
								<p align = "justify">This dataset consists of 1,001 dolphin images from Google Images.</p>
								<h4>Training process</h4>
								<p align = "justify"><i>GPU:</i> Nvidia RTX 2060, <i>learning rate:</i> generator - 0.0002, discriminator - 0.00005, <i>alpha: 0.2</i>, <i>beta</i>: 0.5</p>
								<img src = "./content/dolphins-train.png" width = "100%" height = "100%"/>
								<p align="justify">The dolphins data was acquired from Google Images using the search terms “dolphin jumping out of water”, filtered by the color “blue” and “photography” as the type. An obstacle we faced in attaining “good” results was due to the data being too diversified. Despite our specified query being precise, the returned results contained 1,001 images which showed a diversity of dolphins in different orientations and numbers. We ran a second iteration of this model adjusting the generator and discriminator learning rates from 0.00005 to 0.000005 and 0.0004 to 0.00004, respectively. The output suggested an initial expectation of improved results, but at epoch 500, very little change was observed from the initial iteration.</p>
							</div>

							<div id="horses" class="tabcontent2">
								<h4>The Dataset</h4>
								<p align = "justify">This dataset consists of 1,412 horse images from Google Images.</p>
								<h4>Training process</h4>
								<p align = "justify"><i>GPU:</i> Nvidia RTX 2060, <i>learning rate:</i> generator - 0.0002, discriminator - 0.00005, <i>alpha: 0.5</i>, <i>beta</i>: 0.5</p>
								<img src = "./content/horse-train.png" width = "100%" height = "100%"/>
								<p align="justify">After researching how to train GANs on animal imagery, we noted that GANs are notorious for producing distorted images of animals. This may be due to the complexities and intricacies of animal data (limbs should be correctly shaped and proportional to the rest of the body). After running an initial model which produced poor results, we edited each respective learning rate an additional degree, which did not prove to help the outcome. A potential solution would be to increase the size of the dataset to allow for the model to understand the dataset more clearly. Cleaning and reducing the diversity of images may also prove to be useful in attaining better results.</p>
							</div>

							<div id="macaws" class="tabcontent2">
								<h4>The Dataset</h4>
								<p align = "justify">This dataset consists of 923 macaw images from Google Images.</p>
								<h4>Training process</h4>
								<p align = "justify"><i>GPU:</i> Nvidia RTX 2060, <i>learning rate:</i> generator - 0.0002, discriminator - 0.00005, <i>alpha: 0.2</i>, <i>beta</i>: 0.5</p>
								<img src = "./content/macaw-train.png" width = "100%" height = "100%"/>
								<p align="justify">The macaw dataset was scraped from Google Images using the search terms “red macaw” and filtered by “photography” and the color “red”. 923 images were used to train the model after eliminating anomalous images and rough cleaning of the raw dataset. The model seemingly had issues understanding the intricate details of the macaw face. General color patterns and a broad comprehension of the shape of macaws were evident in the generated output. Tinkering with hyperparameters proved little to no effect for improving the results, and additional data would be needed to provide better context to the model.</p>
							</div>


							<div id="psychedelic" class="tabcontent2">
								<h4>The Dataset</h4>
								<p align = "justify">This dataset consists of 981 psychedelic artwork images from Google Images.</p>
								<h4>Training process</h4>
								<p align = "justify"><i>GPU:</i> Nvidia RTX 2060, <i>learning rate:</i> generator - 0.0002, discriminator - 0.00005, <i>alpha: 0.2</i>, <i>beta</i>: 0.5</p>
								<img src = "./content/psychedelic-train.png" width = "100%" height = "100%"/>
								<p align="justify">The psychedelic dataset was collected through scraping Google Images using the search terms “psychedelic artwork”; 981 images were obtained. Initially, the model seemed to show promise, but eventually ended in mode collapse over a number of epochs. The model was able to gain a general understanding of the number of colors intrinsic to psychedelic artwork, but due to the data being diverse and containing intricate patterns the model was unsuccessful at establishing any patterns or shapes. It would be interesting to try a larger dataset to see how our results would improve. Also, narrowing down to a specific type of psychedelic artwork may have proven useful.</p>
							</div>

							<script>
								document.getElementById('beaches').style.display = "block";
								tabcontent = document.getElementsByClassName("tabcontent2");
								document.getElementById('beachesbutton').style.backgroundColor = "#DCDCDC";

								function openResults2(evt, type) {
								  var i, tabcontent, tablinks;
								  tabcontent = document.getElementsByClassName("tabcontent2");
								  for (i = 0; i < tabcontent.length; i++) {
								    tabcontent[i].style.display = "none";
								    document.getElementById(tabcontent[i].id + "button").style.backgroundColor = 'transparent';
								  }
								  tablinks = document.getElementsByClassName("tablinks");
								  for (i = 0; i < tablinks.length; i++) {
								    tablinks[i].className = tablinks[i].className.replace(" active", "");
								  }
								  document.getElementById(type).style.display = "block";
								  evt.currentTarget.className += " active";
								  document.getElementById(type + "button").style.backgroundColor = "#DCDCDC";
								}
							</script>

						</div>
					</section>

				<!-- Footer -->
					<footer class= "wrapper style1 align-center">
						<div class = "inner">
						<h3>Contact Us</h3>
						<div class="items style3 small">
								<section>
									<b>Claire Pang</b>
									<br><a href="https://www.linkedin.com/in/clairepang/" target = "_blank" class="icon fa-linkedin"></a>&nbsp;<a href="https://www.linkedin.com/in/clairepang/" target = "_blank">clairepang</a>
									<br><i class="icon fa-envelope"></i>&nbsp;clairepang22@gmail.com
									
								</section>
								<section>
									<b>Kyle Astroth</b>
									<br><a href="https://www.linkedin.com/in/kyle-astroth/" target = "_blank" class="icon fa-linkedin"></a>&nbsp;<a href="https://www.linkedin.com/in/kyle-astroth/" target = "_blank">kyle-astroth</a>
									<br><i class="icon fa-envelope"></i>&nbsp;kastroth@mills.edu
								</section>
								<section>
									<b>AJ Bethel</b>
									<br><a href="https://www.linkedin.com/in/ajbethel" target = "_blank" class="icon fa-linkedin"></a>&nbsp;<a href="https://www.linkedin.com/in/ajbethel" target = "_blank">ajbethel</a>
									<br><i class="icon fa-envelope"></i>&nbsp;alwinbethel@gmail.com
								</section></div>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>